{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM For text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = '/Users/divyatmika/Downloads/corpus_new.txt'\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ii.)Use a character-level representation for this model by using extended ASCII that has N = 256 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  410024\n",
      "Total Vocab:  72\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (iii) Window size = 100. (iv.) Read the sequence and (v.) Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  409924\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (v.) (vi) (vii) (viii) : Make a LSTM model with 256 units and activation softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ix.) and (x.) use modelcheckpoint and epochs= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "409924/409924 [==============================] - 2973s 7ms/step - loss: 2.8819\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.88189, saving model to weights-improvement-01-2.8819.hdf5\n",
      "Epoch 2/20\n",
      "409924/409924 [==============================] - 4727s 12ms/step - loss: 2.7112\n",
      "\n",
      "Epoch 00002: loss improved from 2.88189 to 2.71118, saving model to weights-improvement-02-2.7112.hdf5\n",
      "Epoch 3/20\n",
      "409924/409924 [==============================] - 9390s 23ms/step - loss: 2.6103\n",
      "\n",
      "Epoch 00003: loss improved from 2.71118 to 2.61032, saving model to weights-improvement-03-2.6103.hdf5\n",
      "Epoch 4/20\n",
      "409924/409924 [==============================] - 2966s 7ms/step - loss: 2.5081\n",
      "\n",
      "Epoch 00004: loss improved from 2.61032 to 2.50809, saving model to weights-improvement-04-2.5081.hdf5\n",
      "Epoch 5/20\n",
      "409924/409924 [==============================] - 35880s 88ms/step - loss: 2.4155\n",
      "\n",
      "Epoch 00005: loss improved from 2.50809 to 2.41546, saving model to weights-improvement-05-2.4155.hdf5\n",
      "Epoch 6/20\n",
      "409924/409924 [==============================] - 4364s 11ms/step - loss: 2.3392\n",
      "\n",
      "Epoch 00006: loss improved from 2.41546 to 2.33921, saving model to weights-improvement-06-2.3392.hdf5\n",
      "Epoch 7/20\n",
      "409924/409924 [==============================] - 3569s 9ms/step - loss: 2.2833\n",
      "\n",
      "Epoch 00007: loss improved from 2.33921 to 2.28333, saving model to weights-improvement-07-2.2833.hdf5\n",
      "Epoch 8/20\n",
      "409924/409924 [==============================] - 3501s 9ms/step - loss: 2.2292\n",
      "\n",
      "Epoch 00008: loss improved from 2.28333 to 2.22918, saving model to weights-improvement-08-2.2292.hdf5\n",
      "Epoch 9/20\n",
      "409924/409924 [==============================] - 3239s 8ms/step - loss: 2.1843\n",
      "\n",
      "Epoch 00009: loss improved from 2.22918 to 2.18434, saving model to weights-improvement-09-2.1843.hdf5\n",
      "Epoch 10/20\n",
      "409924/409924 [==============================] - 15958s 39ms/step - loss: 5.8125\n",
      "\n",
      "Epoch 00010: loss did not improve from 2.18434\n",
      "Epoch 11/20\n",
      "409924/409924 [==============================] - 3712s 9ms/step - loss: 2.2134\n",
      "\n",
      "Epoch 00011: loss did not improve from 2.18434\n",
      "Epoch 12/20\n",
      "409924/409924 [==============================] - 3714s 9ms/step - loss: 2.1172\n",
      "\n",
      "Epoch 00012: loss improved from 2.18434 to 2.11718, saving model to weights-improvement-12-2.1172.hdf5\n",
      "Epoch 13/20\n",
      "409924/409924 [==============================] - 3337s 8ms/step - loss: 2.0877\n",
      "\n",
      "Epoch 00013: loss improved from 2.11718 to 2.08771, saving model to weights-improvement-13-2.0877.hdf5\n",
      "Epoch 14/20\n",
      "409924/409924 [==============================] - 3186s 8ms/step - loss: 2.0636\n",
      "\n",
      "Epoch 00014: loss improved from 2.08771 to 2.06362, saving model to weights-improvement-14-2.0636.hdf5\n",
      "Epoch 15/20\n",
      "409924/409924 [==============================] - 3388s 8ms/step - loss: 2.0395\n",
      "\n",
      "Epoch 00015: loss improved from 2.06362 to 2.03948, saving model to weights-improvement-15-2.0395.hdf5\n",
      "Epoch 16/20\n",
      "409924/409924 [==============================] - 3745s 9ms/step - loss: 2.0621\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.03948\n",
      "Epoch 17/20\n",
      "409924/409924 [==============================] - 3625s 9ms/step - loss: 1.9929\n",
      "\n",
      "Epoch 00017: loss improved from 2.03948 to 1.99293, saving model to weights-improvement-17-1.9929.hdf5\n",
      "Epoch 18/20\n",
      "165248/409924 [===========>..................] - ETA: 7:27:37 - loss: 1.9769"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model witht the least loss = 1.9769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-17-1.9929.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Text generation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\n",
      "herself lying on the bank, with her head in the lap of her sister who was gently brushing away so siee, and she sabbit said to herself and the sabbit said to herself and the sood way of the was a little that she was a little lad good to the garden, and the sood of the mock turtle said to herself, 'it was a little that the mock turtle said to see it said to sea it said to sea it say it the marge hard sat hn a little that she was so sereated to herself, and she sabbit said to herself, 'it was a little little shated of the sooe of the coomouse it was a little lad good to the little gooder head. and said to herself, 'it was a little little shated of the mouse of the good of the courte, and it was a little little shated in a little that the was a little little shated of the thmee said to see it was a little book of the was a little that she was so sereated to hare a little the began sitee of the was of the was a little that she was so seally and the sabbit was a little lad good to the little gooder head of the gad seared to see it was a little lad good to the little good\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "import sys\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
